#!/bin/bash
# ==============================================================================
# llama.cpp Server å•Ÿå‹•è…³æœ¬
# ==============================================================================
#
# ğŸ“¦ èªªæ˜ï¼š
#   å•Ÿå‹• llama.cpp server è¼‰å…¥ MinerU VLM GGUF æ¨¡å‹
#   æä¾› OpenAI ç›¸å®¹ API ä¾› MinerU vlm-http-client ä½¿ç”¨
#
# ğŸ”§ ä½¿ç”¨æ–¹å¼ï¼š
#   /opt/convertx/start-llama-server.sh
#
# ğŸ“Œ ç’°å¢ƒè®Šæ•¸ï¼š
#   LLAMA_SERVER_HOST - ç›£è½åœ°å€ï¼ˆé è¨­ï¼š127.0.0.1ï¼‰
#   LLAMA_SERVER_PORT - ç›£è½ç«¯å£ï¼ˆé è¨­ï¼š11785ï¼‰
#   LLAMA_CTX_SIZE    - Context sizeï¼ˆé è¨­ï¼š4096ï¼‰
#
# ==============================================================================

set -e

# æ¨¡å‹è·¯å¾‘
VLM_MODEL_DIR="/opt/convertx/models/vlm/mineru2.5-2509-1.2b"
MAIN_MODEL="$VLM_MODEL_DIR/MinerU2.5-2509-1.2B.Q6_K.gguf"
MMPROJ_MODEL="$VLM_MODEL_DIR/MinerU2.5-2509-1.2B.mmproj-Q8_0.gguf"

# ä¼ºæœå™¨è¨­å®š
HOST="${LLAMA_SERVER_HOST:-127.0.0.1}"
PORT="${LLAMA_SERVER_PORT:-11785}"
CTX_SIZE="${LLAMA_CTX_SIZE:-4096}"

# æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
if [ ! -f "$MAIN_MODEL" ]; then
    echo "âŒ æ‰¾ä¸åˆ°ä¸»æ¨¡å‹ï¼š$MAIN_MODEL"
    echo "ğŸ’¡ è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼š./scripts/download-vlm-gguf.sh"
    exit 1
fi

if [ ! -f "$MMPROJ_MODEL" ]; then
    echo "âŒ æ‰¾ä¸åˆ°è¦–è¦ºæŠ•å½±å™¨ï¼š$MMPROJ_MODEL"
    echo "ğŸ’¡ è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼š./scripts/download-vlm-gguf.sh"
    exit 1
fi

# æª¢æŸ¥ llama-server æ˜¯å¦å¯ç”¨
if ! command -v llama-server &> /dev/null; then
    echo "âŒ llama-server æœªå®‰è£"
    exit 1
fi

echo "ğŸš€ å•Ÿå‹• llama.cpp server..."
echo "   ä¸»æ¨¡å‹ï¼š$(basename $MAIN_MODEL)"
echo "   è¦–è¦ºæŠ•å½±å™¨ï¼š$(basename $MMPROJ_MODEL)"
echo "   ç›£è½ï¼š$HOST:$PORT"
echo "   Context Sizeï¼š$CTX_SIZE"
echo ""

exec llama-server \
    -m "$MAIN_MODEL" \
    --mmproj "$MMPROJ_MODEL" \
    --host "$HOST" \
    --port "$PORT" \
    -c "$CTX_SIZE" \
    -ngl 0 \
    --log-disable

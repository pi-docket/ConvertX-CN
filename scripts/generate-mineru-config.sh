#!/bin/bash
# ==============================================================================
# MinerU é…ç½®æª”ç”¢ç”Ÿè…³æœ¬
# ç”¨æ–¼ Docker build éšæ®µç”¢ç”Ÿ mineru.json
# ==============================================================================
#
# ğŸ“¦ é…ç½®èªªæ˜ï¼š
#   - models-dir.pipeline: PDF-Extract-Kit-1.0 è·¯å¾‘ï¼ˆç´” OCR æ¨¡å¼ï¼‰
#   - models-dir.vlm: VLM æ¨¡å‹è·¯å¾‘ï¼ˆé«˜ç²¾åº¦æ¨¡å¼ï¼‰
#   - VLM GGUF æ¨¡å‹é€é llama.cpp server è¼‰å…¥
#   - MinerU ä½¿ç”¨ vlm-http-client é€£æ¥æœ¬åœ° llama.cpp server
#
# ğŸ’¡ æ¶æ§‹èªªæ˜ï¼š
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚   ConvertX      â”‚â”€â”€â”€â”€â–¶â”‚     MinerU      â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#                                    â”‚
#                           vlm-http-client
#                                    â”‚
#                                    â–¼
#                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#                          â”‚  llama.cpp      â”‚
#                          â”‚  (GGUF VLM)     â”‚
#                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
# ==============================================================================

set -e

ARCH=$(uname -m)
MODELS_DIR="${MINERU_MODELS_DIR:-/opt/convertx/models/mineru}"
VLM_MODELS_DIR="/opt/convertx/models/vlm/mineru2.5-2509-1.2b"

mkdir -p /opt/convertx

if [ "$ARCH" = "aarch64" ]; then
    echo '{"models-dir":{"pipeline":"","vlm":""},"model-source":"local","note":"ARM64 - MinerU not supported"}' > /opt/convertx/mineru.json
    cp /opt/convertx/mineru.json /root/mineru.json
    echo "âš ï¸ ARM64ï¼šç”¢ç”Ÿç©ºçš„ mineru.json"
    exit 0
fi

python3 <<'PYTHON'
import json
import os

mineru_models_dir = os.environ.get('MINERU_MODELS_DIR', '/opt/convertx/models/mineru')
vlm_models_dir = '/opt/convertx/models/vlm/mineru2.5-2509-1.2b'

# æª¢æŸ¥ GGUF æ¨¡å‹æ˜¯å¦å­˜åœ¨
gguf_model_path = f'{vlm_models_dir}/MinerU2.5-2509-1.2B.Q6_K.gguf'
mmproj_path = f'{vlm_models_dir}/MinerU2.5-2509-1.2B.mmproj-Q8_0.gguf'

gguf_available = os.path.isfile(gguf_model_path) and os.path.isfile(mmproj_path)

if gguf_available:
    print('âœ… GGUF VLM æ¨¡å‹å·²å°±ç·’ï¼ˆQ6_K é‡åŒ–ç‰ˆï¼‰')
    print(f'   ä¸»æ¨¡å‹: {os.path.basename(gguf_model_path)}')
    print(f'   è¦–è¦ºæŠ•å½±å™¨: {os.path.basename(mmproj_path)}')
    print('')
    print('ğŸ’¡ VLM æ¨¡å¼å°‡é€é llama.cpp server æä¾›ï¼š')
    print('   - llama.cpp server ç›£è½æ–¼ http://127.0.0.1:11785')
    print('   - MinerU ä½¿ç”¨ vlm-http-client å¾Œç«¯é€£æ¥')
else:
    print('âš ï¸ GGUF VLM æ¨¡å‹æœªæ‰¾åˆ°ï¼Œå°‡ä½¿ç”¨ pipeline ç´” OCR æ¨¡å¼')
    print(f'   é æœŸè·¯å¾‘: {gguf_model_path}')
    print('ğŸ’¡ å¦‚éœ€ VLM é«˜ç²¾åº¦æ¨¡å¼ï¼Œè«‹åŸ·è¡Œï¼š')
    print('   ./scripts/download-vlm-gguf.sh')

# MinerU 2.7+ é…ç½®
# ==============================================================================
# ğŸ“Œ é—œéµé…ç½®èªªæ˜ï¼š
#
# 1. models-dir.pipeline: PDF-Extract-Kit-1.0
#    - ç”¨æ–¼ pipeline å¾Œç«¯ï¼ˆç´” OCR æ¨¡å¼ï¼‰
#    - æº–ç¢ºç‡ï¼š82+
#    - ä¸éœ€è¦ GPU
#
# 2. models-dir.vlm: æŒ‡å‘ pipeline è·¯å¾‘ï¼ˆé˜²æ­¢ "not configured" éŒ¯èª¤ï¼‰
#    - MinerU è¨­è¨ˆä¸Šæœƒè®€å–æ­¤è·¯å¾‘ï¼Œå³ä½¿ä¸ä½¿ç”¨ VLM
#    - å¯¦éš› VLM æ¨ç†é€é vlm-http-client é€£æ¥å¤–éƒ¨æœå‹™
#    - é€™è£¡è¨­ç‚º pipeline è·¯å¾‘åªæ˜¯ç‚ºäº†é€šéé…ç½®é©—è­‰
#
# 3. GGUF æ¨¡å‹ï¼š
#    - ç”± llama.cpp server è¼‰å…¥
#    - ä¸èµ° MinerU åŸç”Ÿ transformers è·¯å¾‘
#    - é€é OpenAI ç›¸å®¹ API æä¾›æœå‹™
#
# ==============================================================================
config = {
    'models-dir': {
        'pipeline': f'{mineru_models_dir}/PDF-Extract-Kit-1.0',
        # ğŸ“Œ é‡è¦ï¼šæ­¤è·¯å¾‘å¿…é ˆéç©ºï¼Œå¦å‰‡æœƒè§¸ç™¼ "vlm not configured" éŒ¯èª¤
        # ğŸ“Œ å¯¦éš› VLM æ¨ç†èµ° http-clientï¼Œä¸è®€å–æ­¤è·¯å¾‘çš„æ¨¡å‹
        'vlm': f'{mineru_models_dir}/PDF-Extract-Kit-1.0'
    },
    'model-source': 'local',
    'config_version': '1.3.1',
    # GGUF æ¨¡å‹é…ç½®ï¼ˆä¾›åƒè€ƒï¼Œllama.cpp ä½¿ç”¨ï¼‰
    'gguf-models': {
        'enabled': gguf_available,
        'model': gguf_model_path if gguf_available else '',
        'mmproj': mmproj_path if gguf_available else '',
        'server-url': 'http://127.0.0.1:11785/v1',
        'note': 'GGUF models loaded by llama.cpp server'
    },
    'latex-delimiter-config': {
        'display': {'left': '$$', 'right': '$$'},
        'inline': {'left': '$', 'right': '$'}
    }
}

os.makedirs('/opt/convertx', exist_ok=True)

with open('/opt/convertx/mineru.json', 'w') as f:
    json.dump(config, f, indent=2)

with open('/root/mineru.json', 'w') as f:
    json.dump(config, f, indent=2)

print(f'\nğŸ“‹ mineru.json é…ç½®ï¼š')
print(f'  Pipeline æ¨¡å‹: {config["models-dir"]["pipeline"]}')
print(f'  VLM é…ç½®: {"å·²å•Ÿç”¨ (GGUF + llama.cpp)" if gguf_available else "æœªå•Ÿç”¨ (ä½¿ç”¨ pipeline)"}')
print(f'  Model Source: {config["model-source"]}')
print('âœ… mineru.json å·²ç”¢ç”Ÿ')
PYTHON

echo "âœ… MinerU é…ç½®æª”ç”¢ç”Ÿå®Œæˆ"

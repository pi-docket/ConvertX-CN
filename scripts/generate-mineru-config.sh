#!/bin/bash
# ==============================================================================
# MinerU é…ç½®æª”ç”¢ç”Ÿè…³æœ¬
# ç”¨æ–¼ Docker build éšæ®µç”¢ç”Ÿ mineru.json
# æ”¯æ´ Pipeline æ¨¡å¼ï¼ˆä½¿ç”¨ GGUF é‡åŒ– VLM æ¨¡å‹éœ€æ­é… llama.cppï¼‰
# ==============================================================================
#
# ğŸ“¦ é…ç½®èªªæ˜ï¼š
#   - models-dir.pipeline: PDF-Extract-Kit-1.0 è·¯å¾‘
#   - GGUF VLM æ¨¡å‹ä¸åœ¨æ­¤é…ç½®ï¼Œéœ€ç¨ç«‹å•Ÿå‹• llama.cpp æœå‹™å™¨
#   - MinerU ä½¿ç”¨ pipeline å¾Œç«¯ï¼ˆé è¨­ï¼‰ï¼Œæˆ– vlm-http-client é€£æ¥å¤–éƒ¨æœå‹™
#
# ğŸ’¡ GGUF é‡åŒ–æ¨¡å‹ä½¿ç”¨æ–¹å¼ï¼š
#    1. ä½¿ç”¨ llama.cpp æˆ– ollama è¼‰å…¥ GGUF æ¨¡å‹æä¾› OpenAI API
#    2. è¨­å®šç’°å¢ƒè®Šæ•¸ MINERU_VLM_URL=http://localhost:8080/v1
#    3. MinerU å°‡è‡ªå‹•ä½¿ç”¨ vlm-http-client æ¨¡å¼
#
# ==============================================================================

set -e

ARCH=$(uname -m)
MODELS_DIR="${MINERU_MODELS_DIR:-/opt/convertx/models/mineru}"

mkdir -p /opt/convertx

if [ "$ARCH" = "aarch64" ]; then
    echo '{"models-dir":{"pipeline":"","vlm":""},"model-source":"local","note":"ARM64 - MinerU not supported"}' > /opt/convertx/mineru.json
    cp /opt/convertx/mineru.json /root/mineru.json
    echo "âš ï¸ ARM64ï¼šç”¢ç”Ÿç©ºçš„ mineru.json"
    exit 0
fi

python3 <<'PYTHON'
import json
import os

mineru_models_dir = os.environ.get('MINERU_MODELS_DIR', '/opt/convertx/models/mineru')

# æª¢æŸ¥ GGUF æ¨¡å‹æ˜¯å¦å­˜åœ¨
gguf_dir = f'{mineru_models_dir}/MinerU-VLM-GGUF'
gguf_model_path = f'{gguf_dir}/MinerU2.5-2509-1.2B.Q8_0.gguf'
mmproj_path = f'{gguf_dir}/mmproj-MinerU2.5-2509-1.2B-f16.gguf'

gguf_available = os.path.isfile(gguf_model_path) and os.path.isfile(mmproj_path)

if gguf_available:
    print('âœ… GGUF VLM æ¨¡å‹å·²å°±ç·’')
    print(f'   ä¸»æ¨¡å‹: {gguf_model_path}')
    print(f'   è¦–è¦ºæŠ•å½±å™¨: {mmproj_path}')
    print('')
    print('ğŸ’¡ ä½¿ç”¨ GGUF æ¨¡å‹çš„æ­¥é©Ÿï¼š')
    print('   1. å•Ÿå‹• llama.cpp æœå‹™å™¨ï¼š')
    print(f'      llama-server -m {gguf_model_path} --mmproj {mmproj_path} --port 8080')
    print('   2. è¨­å®š MinerU ä½¿ç”¨ http-client æ¨¡å¼ï¼š')
    print('      mineru -p input.pdf -o output -b vlm-http-client -u http://localhost:8080/v1')
else:
    print('âš ï¸ GGUF VLM æ¨¡å‹æœªæ‰¾åˆ°')
    print('ğŸ’¡ å°‡ä½¿ç”¨ pipeline æ¨¡å¼ï¼ˆç´” OCRï¼Œä¸ä½¿ç”¨ VLMï¼‰')

# MinerU 2.7+ é…ç½®
# ğŸ“Œ é‡è¦ï¼šç”±æ–¼ä½¿ç”¨ GGUF æ¨¡å‹ï¼ŒVLM è·¯å¾‘ç•™ç©º
# ğŸ“Œ MinerU å°‡é è¨­ä½¿ç”¨ pipeline å¾Œç«¯
# ğŸ“Œ è‹¥è¦ä½¿ç”¨ VLMï¼Œéœ€æ‰‹å‹•å•Ÿå‹• llama.cpp ä¸¦ä½¿ç”¨ http-client æ¨¡å¼
config = {
    'models-dir': {
        'pipeline': f'{mineru_models_dir}/PDF-Extract-Kit-1.0',
        'vlm': ''  # GGUF æ¨¡å‹ä¸ç›´æ¥åœ¨æ­¤é…ç½®
    },
    'model-source': 'local',
    'config_version': '1.3.1',
    'gguf_models': {
        'model': gguf_model_path if gguf_available else '',
        'mmproj': mmproj_path if gguf_available else '',
        'note': 'GGUF models for llama.cpp server, not native MinerU'
    },
    'latex-delimiter-config': {
        'display': {'left': '@@', 'right': '@@'},
        'inline': {'left': '@', 'right': '@'}
    }
}

os.makedirs('/opt/convertx', exist_ok=True)

with open('/opt/convertx/mineru.json', 'w') as f:
    json.dump(config, f, indent=2)

with open('/root/mineru.json', 'w') as f:
    json.dump(config, f, indent=2)

print(f'\nğŸ“‹ mineru.json é…ç½®ï¼š')
print(f'  Pipeline: {config["models-dir"]["pipeline"]}')
print(f'  Model Source: {config["model-source"]}')
if gguf_available:
    print(f'  GGUF Model: {os.path.basename(gguf_model_path)} (éœ€æ­é… llama.cpp)')
print('âœ… mineru.json å·²ç”¢ç”Ÿ')
PYTHON

echo "âœ… MinerU é…ç½®æª”ç”¢ç”Ÿå®Œæˆ"

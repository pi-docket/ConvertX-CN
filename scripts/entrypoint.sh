#!/bin/bash
# ==============================================================================
# ConvertX-CN å•Ÿå‹•è…³æœ¬ï¼ˆåŒ…å« llama.cpp VLM serverï¼‰
# ==============================================================================
#
# ğŸ“¦ èªªæ˜ï¼š
#   æ­¤è…³æœ¬ä½œç‚º Docker å®¹å™¨çš„å…¥å£é»
#   1. æª¢æŸ¥ä¸¦å•Ÿå‹• llama.cpp serverï¼ˆèƒŒæ™¯é‹è¡Œï¼‰
#   2. ç­‰å¾… server å°±ç·’
#   3. å•Ÿå‹•ä¸»æ‡‰ç”¨ç¨‹å¼
#
# ğŸ“Œ ç’°å¢ƒè®Šæ•¸ï¼š
#   MINERU_BACKEND     - å¾Œç«¯é¸æ“‡ï¼ˆvlm-http-client / pipelineï¼‰
#   LLAMA_SERVER_PORT  - llama.cpp ç«¯å£ï¼ˆé è¨­ï¼š11785ï¼‰
#   SKIP_LLAMA_SERVER  - è¨­ç‚º 1 è·³é llama.cpp å•Ÿå‹•
#
# ==============================================================================

set -e

# é¡è‰²è¼¸å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[OK]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# ==============================================================================
# VLM Server å•Ÿå‹•é‚è¼¯
# ==============================================================================
start_vlm_server() {
    local PORT="${LLAMA_SERVER_PORT:-11785}"
    local HOST="${LLAMA_SERVER_HOST:-127.0.0.1}"
    local MODEL="${VLM_GGUF_MODEL:-/opt/convertx/models/vlm/mineru2.5-2509-1.2b/MinerU2.5-2509-1.2B.Q6_K.gguf}"
    local MMPROJ="${VLM_GGUF_MMPROJ:-/opt/convertx/models/vlm/mineru2.5-2509-1.2b/MinerU2.5-2509-1.2B.mmproj-Q8_0.gguf}"
    
    # æª¢æŸ¥æ˜¯å¦æ‡‰è©²å•Ÿå‹•
    if [ "${SKIP_LLAMA_SERVER}" = "1" ]; then
        log_warn "SKIP_LLAMA_SERVER=1ï¼Œè·³é llama.cpp server"
        return 0
    fi
    
    # æª¢æŸ¥å¾Œç«¯è¨­å®š
    if [ "${MINERU_BACKEND}" != "vlm-http-client" ] && [ "${MINERU_BACKEND}" != "hybrid-http-client" ]; then
        log_info "MINERU_BACKEND=${MINERU_BACKEND}ï¼Œä¸éœ€è¦ llama.cpp server"
        return 0
    fi
    
    # æª¢æŸ¥ llama-server æ˜¯å¦å¯ç”¨
    if ! command -v llama-server &> /dev/null; then
        log_error "llama-server æœªå®‰è£"
        log_warn "å›é€€åˆ° pipeline æ¨¡å¼"
        export MINERU_BACKEND="pipeline"
        unset MINERU_VLM_URL
        return 0
    fi
    
    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆ
    if [ ! -f "$MODEL" ]; then
        log_error "æ‰¾ä¸åˆ° VLM ä¸»æ¨¡å‹ï¼š$MODEL"
        log_warn "å›é€€åˆ° pipeline æ¨¡å¼"
        export MINERU_BACKEND="pipeline"
        unset MINERU_VLM_URL
        return 0
    fi
    
    if [ ! -f "$MMPROJ" ]; then
        log_error "æ‰¾ä¸åˆ°è¦–è¦ºæŠ•å½±å™¨ï¼š$MMPROJ"
        log_warn "å›é€€åˆ° pipeline æ¨¡å¼"
        export MINERU_BACKEND="pipeline"
        unset MINERU_VLM_URL
        return 0
    fi
    
    log_info "å•Ÿå‹• llama.cpp VLM server..."
    log_info "  ä¸»æ¨¡å‹ï¼š$(basename $MODEL)"
    log_info "  è¦–è¦ºæŠ•å½±å™¨ï¼š$(basename $MMPROJ)"
    log_info "  ç›£è½ï¼š$HOST:$PORT"
    
    # èƒŒæ™¯å•Ÿå‹• llama.cpp server
    llama-server \
        -m "$MODEL" \
        --mmproj "$MMPROJ" \
        --host "$HOST" \
        --port "$PORT" \
        -c "${LLAMA_CTX_SIZE:-4096}" \
        -ngl 0 \
        --log-disable \
        2>&1 | while read -r line; do echo "[llama] $line"; done &
    
    LLAMA_PID=$!
    
    # ç­‰å¾… server å°±ç·’ï¼ˆæœ€å¤š 60 ç§’ï¼‰
    log_info "ç­‰å¾… llama.cpp server å°±ç·’..."
    local MAX_WAIT=60
    local WAITED=0
    
    while [ $WAITED -lt $MAX_WAIT ]; do
        if curl -s "http://$HOST:$PORT/health" > /dev/null 2>&1; then
            log_success "llama.cpp server å·²å°±ç·’ (http://$HOST:$PORT)"
            return 0
        fi
        
        # æª¢æŸ¥é€²ç¨‹æ˜¯å¦é‚„åœ¨é‹è¡Œ
        if ! kill -0 $LLAMA_PID 2>/dev/null; then
            log_error "llama.cpp server å•Ÿå‹•å¤±æ•—"
            log_warn "å›é€€åˆ° pipeline æ¨¡å¼"
            export MINERU_BACKEND="pipeline"
            unset MINERU_VLM_URL
            return 0
        fi
        
        sleep 1
        WAITED=$((WAITED + 1))
        
        if [ $((WAITED % 10)) -eq 0 ]; then
            log_info "  ç­‰å¾…ä¸­... (${WAITED}s)"
        fi
    done
    
    log_error "llama.cpp server å•Ÿå‹•è¶…æ™‚ï¼ˆ${MAX_WAIT}sï¼‰"
    log_warn "å›é€€åˆ° pipeline æ¨¡å¼"
    export MINERU_BACKEND="pipeline"
    unset MINERU_VLM_URL
    
    # å˜—è©¦çµ‚æ­¢æ®­å±é€²ç¨‹
    kill $LLAMA_PID 2>/dev/null || true
}

# ==============================================================================
# ä¸»ç¨‹å¼
# ==============================================================================
main() {
    echo "=============================================="
    echo "  ConvertX-CN å•Ÿå‹•ä¸­..."
    echo "=============================================="
    echo ""
    
    # é¡¯ç¤ºé…ç½®
    log_info "MinerU å¾Œç«¯ï¼š${MINERU_BACKEND:-pipeline}"
    log_info "VLM URLï¼š${MINERU_VLM_URL:-æœªè¨­å®š}"
    
    # å•Ÿå‹• VLM serverï¼ˆå¦‚æœéœ€è¦ï¼‰
    start_vlm_server
    
    echo ""
    log_info "æœ€çµ‚å¾Œç«¯è¨­å®šï¼š${MINERU_BACKEND}"
    echo ""
    
    # å•Ÿå‹•ä¸»æ‡‰ç”¨ç¨‹å¼
    log_info "å•Ÿå‹• ConvertX-CN ä¸»ç¨‹å¼..."
    exec bun run dist/src/index.js "$@"
}

# åŸ·è¡Œ
main "$@"
